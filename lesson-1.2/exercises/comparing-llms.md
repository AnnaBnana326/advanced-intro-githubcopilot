# Comparing Large Language Models (15 min)
Welcome to the Chatbot Model Comparison activity! Here, you will compare the output of three different chatbot models. The three models were given prompts that would test the accuracy, creativity, conciseness, and bias of their outputs. Your job is to select the model that performed best in each category. Let's get started!

Complete the activity [here](https://igfnaqfcyl-13589482-i.codehs.me/index.html).  Then edit this page and write down your reflections here:

### Which model did you find performed best overall, and why?
Llama 3 because it provided elaboration on each, but wasn't long-winded.

### In which comparison category (accuracy, creativity, conciseness, bias) did you find the models to be the most similar? What about the most different?
The most similar was the moons because the prompt had a factual answer, while the others didn't have a "correct" answer. The largest difference was the words chosen to describe a place they've never been. Two of them shared the word "wander," which I thought was interesting, but none of them produced the same word.

### Were you surprised by any of the results?
I was surprised the creativity prompt gave 3 different words.

### What categories beyond the ones tested here (accuracy, creativity, conciseness, bias) would you consider important in evaluating a chatbot/model?
I would consider the sources if they can be provided. Some AI will pull from Reddit, Quizlet, etc, which is not factual. 
